{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## RAG Pipeline Notebook\n",
    "\n",
    "This notebook leverages the following\n",
    "* Doc parsing using Langchain parser\n",
    "* Embedding using the service from VCF \n",
    "* LLM using the service from VCF\n",
    "* Primitive chatbot interface using gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adcc85cf-4c2b-48d8-b662-4e313c01ebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atlassian-python-api in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (3.41.16)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: deprecated in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (1.2.14)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (1.16.0)\n",
      "Requirement already satisfied: oauthlib in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (2.0.0)\n",
      "Requirement already satisfied: jmespath in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from atlassian-python-api) (4.12.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from beautifulsoup4->atlassian-python-api) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from deprecated->atlassian-python-api) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->atlassian-python-api) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->atlassian-python-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->atlassian-python-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests->atlassian-python-api) (2024.8.30)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet  vllm -q\n",
    "%pip install atlassian-python-api pytesseract Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_loaders import PlaywrightURLLoader\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from langchain_community.document_loaders import ConfluenceLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "# MODEL = \"gpt-4o-mini\"\n",
    "MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "# os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('VMWARE_LLM_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['CONFLUENCE_API_KEY'] = os.getenv('CONFLUENCE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d865aef-118e-4c4f-939e-6edd3045eb17",
   "metadata": {},
   "source": [
    "## Load the PDF docs from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5259c2-7bd4-46c8-9f0c-2d08ddd0186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs from a directory. Expect everything to be PDFs\n",
    "def load_dir(dirpath):\n",
    "    data = []\n",
    "    for fname in glob.glob(os.path.join(dirpath, \"*\")):\n",
    "        loader = PyPDFLoader(file_path=fname)\n",
    "        data.extend(loader.load())\n",
    "    return data\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n===\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Load data\n",
    "data = load_dir(\"security_docs\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b946b84-84a5-4494-8cda-d650d7bc64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f894a3f-6c9d-43d5-9f38-6a3f7259e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e63b6-3911-421c-ad90-86afb6d017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35c35a-c043-4c8d-8673-467d6d6dad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "# print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0672ab1-1657-41ef-aa78-a5a48ad4cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[99])\n",
    "print(\"==\")\n",
    "print(chunks[100])\n",
    "print(\"==\")\n",
    "print(chunks[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede3d7a-7bfb-4287-8c72-a8488c2afc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([chunk.metadata['source'] for chunk in chunks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fe264-ef1a-47e2-a30c-3ce55fc10c70",
   "metadata": {},
   "source": [
    "## Load the confluence page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7922699-112e-4127-83b1-f7d4d051fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://vmw-confluence.broadcom.net/pages/viewpage.action?pageId=1481319163\"\"\"\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://vmw-confluence.broadcom.net\", token=os.environ['CONFLUENCE_API_KEY'],\n",
    "    # space_key=\"VELOENG\", \n",
    "    page_ids=[\"1481319163\"],\n",
    "    include_attachments=True, \n",
    "    limit=50,\n",
    "    keep_markdown_format=True\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd745276-5674-4761-a96b-f5af35750138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363ca6a-6d8f-4dcf-8a44-67cbe008c92a",
   "metadata": {},
   "source": [
    "## Load the web page recursively\n",
    "\n",
    "Try to parse the VMware External KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86320d0c-43e1-4b3e-89a3-7b0d83ff149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "    \n",
    "loader = RecursiveUrlLoader(\n",
    "    # \"https://docs.python.org/3.9/\",\n",
    "    \"https://support.broadcom.com/web/ecx/search?searchString=velocloud&activeType=all&from=0&sortby=_score&orderBy=desc&pageNo=1&aggregations=%5B%7B%22type%22%3A%22_type%22%2C%22filter%22%3A%5B%22knowledge_articles_doc%22%5D%7D%2C%7B%22type%22%3A%22productname%22%2C%22filter%22%3A%5B%22VMware+VeloCloud+SD-WAN%22%2C%22VMware+SD-WAN+by+VeloCloud%22%5D%7D%5D&uid=d042dbba-f8c4-11ea-beba-0242ac12000b&resultsPerPage=10&exactPhrase=SD-WAN&withOneOrMore=&withoutTheWords=&pageSize=10&language=en&state=15&suCaseCreate=false\",\n",
    "    extractor=bs4_extractor\n",
    "    # max_depth=2,\n",
    "    # use_async=False,\n",
    "    # extractor=bs4_extractor,\n",
    "    # metadata_extractor=None,\n",
    "    # exclude_dirs=(),\n",
    "    # timeout=10,\n",
    "    # check_response_status=True,\n",
    "    # continue_on_failure=True,\n",
    "    # prevent_outside=True,\n",
    "    # base_url=None,\n",
    "    # ...\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a76ea-00e5-4a92-83fe-9dba1033a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
    "\n",
    "### Sidenote\n",
    "\n",
    "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"BAAI/bge-en-icl\", \n",
    "    base_url=\"https://llm.ai.broadcom.net/api/v1\", \n",
    "    tiktoken_enabled=False,\n",
    "    model_kwargs={\"encoding_format\": \"float\"}\n",
    ")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework (with thanks to Jon R for identifying and fixing a bug in this!)\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "# print(set(metadatas)\n",
    "doc_types = [metadata['source'] for metadata in metadatas]\n",
    "# print(set(doc_types))\n",
    "color_code = colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#17becf']\n",
    "colors = [list(set(doc_types)).index(t) for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d89a07-e6ca-4037-b30e-648bb01d21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D FAISS Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Use LangChain to bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7, \n",
    "    model=MODEL, \n",
    "    base_url=\"https://llm.ai.broadcom.net/api/v1\"\n",
    ")\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19c10e-97dd-44cb-b2fa-ab586aaa073b",
   "metadata": {},
   "source": [
    "### Option 1 - Use the ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe611e9-57c3-41b2-acac-aef884960f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa4c84-e750-4f45-b842-8a09b76d6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a9cb2-1a0b-4d74-820f-5d48bf8cfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f0db0-51b9-42a4-ad81-8b8e8a710b9d",
   "metadata": {},
   "source": [
    "### Option 2 - Use the langchain LCEL (|) style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef296a7-9e51-4e26-80a0-e5a2b134d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\n",
    "You are a professional technical writer. Use the following pieces of retrieved context to answer the question at the end. Be as verbose and comprehensive as possible.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "QUESTION = \"Write a industry grade technical white paper on Security in VeloCloud which will cover the following topics : 1. Security components on the edge, in the cloud, on SODA 2. What threats do we protect against and how 3. What special constructs do we have to reduce the attack surface 4. How does our security telemetry work 5. What are examples of Velocloud deployments that have been shown to be secure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b53d6-4c26-47e6-b642-db699c1b3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Setup system prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(template=RAG_TEMPLATE)\n",
    "\n",
    "# Setup callback so we can see the trace\n",
    "handler = StdOutCallbackHandler()\n",
    "config = {\n",
    " 'callbacks' : [handler]\n",
    "}\n",
    "\n",
    "# Setup chain\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run and get output\n",
    "response = qa_chain.invoke(QUESTION, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6547b8-1160-4b21-abc2-d8d43db3ee5e",
   "metadata": {},
   "source": [
    "### Option 3 - Use LCEL and with debug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce50c5e-65ac-40c6-95f0-01968cf6d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"You are a professional technical writer. Use the following pieces of retrieved context to answer the question at the end. Be as verbose and comprehensive as possible:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based only on the context provided. If you cannot find the answer in the context, say \"I cannot answer this question based on the provided context.\"\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "class QueryDebugger:\n",
    "    def __init__(self):\n",
    "        self.retrieved_docs = []\n",
    "        self.final_prompt = None\n",
    "        self.token_usage = None\n",
    "        self.cost = None\n",
    "\n",
    "    def clear(self):\n",
    "        self.retrieved_docs = []\n",
    "        self.final_prompt = None\n",
    "        self.token_usage = None\n",
    "        self.cost = None\n",
    "\n",
    "debugger = QueryDebugger()\n",
    "\n",
    "# Modified RAG chain with debugging\n",
    "def debug_chain(question):\n",
    "    # Clear previous debug info\n",
    "    debugger.clear()\n",
    "    \n",
    "    # Get retrieved documents and construct prompt\n",
    "    retriever_output = retriever.invoke(question)\n",
    "    debugger.retrieved_docs = retriever_output\n",
    "    \n",
    "    # Log retrieved documents\n",
    "    logger.info(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(retriever_output, 1):\n",
    "        logger.info(f\"\\nDocument {i}:\")\n",
    "        logger.info(f\"Content: {doc.page_content[:200]}...\")\n",
    "        logger.info(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    \n",
    "    # Construct and log the prompt\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retriever_output)\n",
    "    prompt_args = {\"context\": context, \"question\": question}\n",
    "    final_prompt = prompt.format(**prompt_args)\n",
    "    debugger.final_prompt = final_prompt\n",
    "    logger.info(\"\\nFinal Prompt:\")\n",
    "    logger.info(final_prompt)\n",
    "    \n",
    "    # Execute LLM call with token tracking\n",
    "    with get_openai_callback() as cb:\n",
    "        response = rag_chain.invoke(question)\n",
    "        debugger.token_usage = {\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"total_tokens\": cb.total_tokens\n",
    "        }\n",
    "        debugger.cost = cb.total_cost\n",
    "    \n",
    "    # Log token usage and cost\n",
    "    logger.info(\"\\nToken Usage:\")\n",
    "    logger.info(f\"Prompt tokens: {debugger.token_usage['prompt_tokens']}\")\n",
    "    logger.info(f\"Completion tokens: {debugger.token_usage['completion_tokens']}\")\n",
    "    logger.info(f\"Total tokens: {debugger.token_usage['total_tokens']}\")\n",
    "    logger.info(f\"Total cost: ${debugger.cost:.4f}\")\n",
    "    \n",
    "    return response, debugger\n",
    "\n",
    "# Gradio interface function\n",
    "def process_query(question):\n",
    "    \"\"\"Process the user query through the RAG pipeline\"\"\"\n",
    "    try:\n",
    "        response, debug_info = debug_chain(question)\n",
    "        \n",
    "        # Format debug information for display\n",
    "        debug_output = f\"\"\"\n",
    "=== Debug Information ===\n",
    "\n",
    "Retrieved Documents:\n",
    "{'-' * 50}\n",
    "{\"\".join(f'Document {i+1}:\\n{doc.page_content[:200]}...\\n\\n' for i, doc in enumerate(debug_info.retrieved_docs))}\n",
    "\n",
    "Final Prompt:\n",
    "{'-' * 50}\n",
    "{debug_info.final_prompt}\n",
    "\n",
    "Token Usage:\n",
    "{'-' * 50}\n",
    "Prompt tokens: {debug_info.token_usage['prompt_tokens']}\n",
    "Completion tokens: {debug_info.token_usage['completion_tokens']}\n",
    "Total tokens: {debug_info.token_usage['total_tokens']}\n",
    "Estimated cost: ${debug_info.cost:.4f}\n",
    "\n",
    "Answer:\n",
    "{'-' * 50}\n",
    "{response}\n",
    "\"\"\"\n",
    "        return debug_output\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {str(e)}\")\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_query,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=2,\n",
    "        placeholder=\"Enter your question here...\",\n",
    "        label=\"Question\"\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        lines=20,\n",
    "        label=\"Answer with Debug Information\"\n",
    "    ),\n",
    "    title=\"RAG Question-Answering System with Debug Information\",\n",
    "    description=\"Ask questions about your documents using RAG (Retrieval-Augmented Generation)\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad19ef7-e019-4daf-a59b-6c3fb3c40df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
