{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1957f5cb",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Weaviate\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f0986",
   "metadata": {},
   "source": [
    "# Weaviate\n",
    "\n",
    "This notebook covers how to get started with the Weaviate vector store in LangChain, using the `langchain-weaviate` package.\n",
    "\n",
    "> [Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects.\n",
    "\n",
    "To use this integration, you need to have a running Weaviate database instance.\n",
    "\n",
    "## Minimum versions\n",
    "\n",
    "This module requires Weaviate `1.23.7` or higher. However, we recommend you use the latest version of Weaviate.\n",
    "\n",
    "## Connecting to Weaviate\n",
    "\n",
    "In this notebook, we assume that you have a local instance of Weaviate running on `http://localhost:8080` and port 50051 open for [gRPC traffic](https://weaviate.io/blog/grpc-performance-improvements). So, we will connect to Weaviate with:\n",
    "\n",
    "```python\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "```\n",
    "\n",
    "### Other deployment options\n",
    "\n",
    "Weaviate can be [deployed in many different ways](https://weaviate.io/developers/weaviate/starter-guides/which-weaviate) such as using [Weaviate Cloud Services (WCS)](https://console.weaviate.cloud), [Docker](https://weaviate.io/developers/weaviate/installation/docker-compose) or [Kubernetes](https://weaviate.io/developers/weaviate/installation/kubernetes). \n",
    "\n",
    "If your Weaviate instance is deployed in another way, [read more here](https://weaviate.io/developers/weaviate/client-libraries/python#instantiate-a-client) about different ways to connect to Weaviate. You can use different [helper functions](https://weaviate.io/developers/weaviate/client-libraries/python#python-client-v4-helper-functions) or [create a custom instance](https://weaviate.io/developers/weaviate/client-libraries/python#python-client-v4-explicit-connection).\n",
    "\n",
    "> Note that you require a `v4` client API, which will create a `weaviate.WeaviateClient` object.\n",
    "\n",
    "### Authentication\n",
    "\n",
    "Some Weaviate instances, such as those running on WCS, have authentication enabled, such as API key and/or username+password authentication.\n",
    "\n",
    "Read the [client authentication guide](https://weaviate.io/developers/weaviate/client-libraries/python#authentication) for more information, as well as the [in-depth authentication configuration page](https://weaviate.io/developers/weaviate/configuration/authentication)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8437b1",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97b55c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (1.52.1)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -Uqq langchain-weaviate\n",
    "%pip install openai tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdc060",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This notebook uses the OpenAI API through `OpenAIEmbeddings`. We suggest obtaining an OpenAI API key and export it as an environment variable with the name `OPENAI_API_KEY`.\n",
    "\n",
    "Once this is done, your OpenAI API key will be read automatically. If you are new to environment variables, read more about them [here](https://docs.python.org/3/library/os.html#os.environ) or in [this guide](https://www.twilio.com/en-us/blog/environment-variables-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc10a14-2819-4aca-b541-4359d57c8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3a83f",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efee7cd",
   "metadata": {},
   "source": [
    "## Find objects by similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37144c-208d-4ab3-9f3a-0407a69fe052",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an example of how to find objects by similarity to a query, from data import to querying the Weaviate instance.\n",
    "\n",
    "### Step 1: Data import\n",
    "\n",
    "First, we will create data to add to `Weaviate` by loading and chunking the contents of a long text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0ab00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4618779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a22ab0-30d0-4928-8a82-006f3f999b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x107d9f8d0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x11109fb90> model='text-embedding-ada-002' dimensions=None deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae774cf5",
   "metadata": {},
   "source": [
    "Now, we can import the data. \n",
    "\n",
    "To do so, connect to the Weaviate instance and use the resulting `weaviate_client` object. For example, we can import the documents as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbda8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06f64b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "WeaviateGRPCUnavailableError",
     "evalue": "\nWeaviate v1.27.3 makes use of a high-speed gRPC API as well as a REST API.\nUnfortunately, the gRPC health check against Weaviate could not be completed.\n\nThis error could be due to one of several reasons:\n- The gRPC traffic at the specified port is blocked by a firewall.\n- gRPC is not enabled or incorrectly configured on the server or the client.\n    - Please check that the server address and port (weaviate:50051) are correct.\n- your connection is unstable or has a high latency. In this case you can:\n    - increase init-timeout in `weaviate.connect_to_local(additional_config=wvc.init.AdditionalConfig(timeout=wvc.init.Timeout(init=X)))`\n    - disable startup checks by connecting using `skip_init_checks=True`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAioRpcError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/v4.py:700\u001b[0m, in \u001b[0;36mConnectionV4._ping_grpc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     res: health_pb2\u001b[38;5;241m.\u001b[39mHealthCheckResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel\u001b[38;5;241m.\u001b[39munary_unary(\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/grpc.health.v1.Health/Check\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    702\u001b[0m         request_serializer\u001b[38;5;241m=\u001b[39mhealth_pb2\u001b[38;5;241m.\u001b[39mHealthCheckRequest\u001b[38;5;241m.\u001b[39mSerializeToString,\n\u001b[1;32m    703\u001b[0m         response_deserializer\u001b[38;5;241m=\u001b[39mhealth_pb2\u001b[38;5;241m.\u001b[39mHealthCheckResponse\u001b[38;5;241m.\u001b[39mFromString,\n\u001b[1;32m    704\u001b[0m     )(health_pb2\u001b[38;5;241m.\u001b[39mHealthCheckRequest(), timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_config\u001b[38;5;241m.\u001b[39minit)\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m health_pb2\u001b[38;5;241m.\u001b[39mHealthCheckResponse\u001b[38;5;241m.\u001b[39mSERVING:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/grpc/aio/_call.py:327\u001b[0m, in \u001b[0;36m_UnaryResponseMixin.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _create_rpc_error(\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_call\u001b[38;5;241m.\u001b[39m_initial_metadata,\n\u001b[1;32m    329\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_call\u001b[38;5;241m.\u001b[39m_status,\n\u001b[1;32m    330\u001b[0m         )\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAioRpcError\u001b[0m: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.DEADLINE_EXCEEDED\n\tdetails = \"Deadline Exceeded\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2024-11-17T20:09:47.170661-08:00\", grpc_status:4, grpc_message:\"Deadline Exceeded\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWeaviateGRPCUnavailableError\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weaviate_client \u001b[38;5;241m=\u001b[39m \u001b[43mweaviate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweaviate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8080\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrpc_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50051\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weaviate_client\u001b[38;5;241m.\u001b[39mis_ready():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeaviate connection succeessful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/helpers.py:209\u001b[0m, in \u001b[0;36mconnect_to_local\u001b[0;34m(host, port, grpc_port, headers, additional_config, skip_init_checks, auth_credentials)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect_to_local\u001b[39m(\n\u001b[1;32m    150\u001b[0m     host: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m     port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8080\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     auth_credentials: Optional[AuthCredentials] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m WeaviateClient:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Connect to a local Weaviate instance deployed using Docker compose with standard port configurations.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        >>> # The connection is automatically closed when the context is exited.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWeaviateClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconnection_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConnectionParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProtocolParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrpc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProtocolParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrpc_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43madditional_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_init_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_init_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauth_client_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/helpers.py:410\u001b[0m, in \u001b[0;36m__connect\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    409\u001b[0m     client\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/helpers.py:406\u001b[0m, in \u001b[0;36m__connect\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__connect\u001b[39m(client: WeaviateClient) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m WeaviateClient:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m client\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/syncify.py:23\u001b[0m, in \u001b[0;36mconvert.<locals>.sync_method\u001b[0;34m(self, __new_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msync_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, __new_name\u001b[38;5;241m=\u001b[39mnew_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     async_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, __new_name)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_EventLoopSingleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/event_loop.py:40\u001b[0m, in \u001b[0;36m_EventLoop.run_until_complete\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateClosedClientError()\n\u001b[1;32m     39\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/client_base.py:152\u001b[0m, in \u001b[0;36m_WeaviateClientBase.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_connected():\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_init_checks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/v4.py:181\u001b[0m, in \u001b[0;36mConnectionV4.connect\u001b[0;34m(self, skip_init_checks)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/v4.py:178\u001b[0m, in \u001b[0;36mConnectionV4.connect\u001b[0;34m(self, skip_init_checks)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_init_checks:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ping_grpc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__check_package_version())\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.12/site-packages/weaviate/connect/v4.py:710\u001b[0m, in \u001b[0;36mConnectionV4._ping_grpc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WeaviateGRPCUnavailableError(\n\u001b[1;32m    707\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_params\u001b[38;5;241m.\u001b[39m_grpc_address\n\u001b[1;32m    708\u001b[0m         )\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateGRPCUnavailableError(\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_params\u001b[38;5;241m.\u001b[39m_grpc_address\n\u001b[1;32m    712\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mWeaviateGRPCUnavailableError\u001b[0m: \nWeaviate v1.27.3 makes use of a high-speed gRPC API as well as a REST API.\nUnfortunately, the gRPC health check against Weaviate could not be completed.\n\nThis error could be due to one of several reasons:\n- The gRPC traffic at the specified port is blocked by a firewall.\n- gRPC is not enabled or incorrectly configured on the server or the client.\n    - Please check that the server address and port (weaviate:50051) are correct.\n- your connection is unstable or has a high latency. In this case you can:\n    - increase init-timeout in `weaviate.connect_to_local(additional_config=wvc.init.AdditionalConfig(timeout=wvc.init.Timeout(init=X)))`\n    - disable startup checks by connecting using `skip_init_checks=True`\n"
     ]
    }
   ],
   "source": [
    "weaviate_client = weaviate.connect_to_local(\n",
    "    host=\"weaviate\",\n",
    "    port=8080,\n",
    "    grpc_port=50051\n",
    ")\n",
    "if weaviate_client.is_ready():\n",
    "    print(\"Weaviate connection succeessful\")\n",
    "else:\n",
    "    print(\"Weaviate connection fail\")\n",
    "db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ac6f73-bebb-44f3-be59-1a366acc9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x1132159d0>\n"
     ]
    }
   ],
   "source": [
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe29115",
   "metadata": {},
   "source": [
    "### Step 2: Perform the search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799f5a3",
   "metadata": {},
   "source": [
    "We can now perform a similarity search. This will return the most similar documents to the query text, based on the embeddings stored in Weaviate and an equivalent embedding generated from the query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc3aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Ac...\n",
      "\n",
      "Document 2:\n",
      "A former top litigator in private practice. A former federal public defender. And from a family of p...\n",
      "\n",
      "Document 3:\n",
      "Vice President Harris and I ran for office with a new economic vision for America. \n",
      "\n",
      "Invest in Ameri...\n",
      "\n",
      "Document 4:\n",
      "And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaugh...\n"
     ]
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Print the first 100 characters of each result\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1134ef",
   "metadata": {},
   "source": [
    "You can also add filters, which will either include or exclude results based on the filter conditions. (See [more filter examples](https://weaviate.io/developers/weaviate/search/filters).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1210f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "for filter_str in [\"blah.txt\", \"state_of_the_union.txt\"]:\n",
    "    search_filter = Filter.by_property(\"source\").equal(filter_str)\n",
    "    filtered_search_results = db.similarity_search(query, filters=search_filter)\n",
    "    print(len(filtered_search_results))\n",
    "    if filter_str == \"state_of_the_union.txt\":\n",
    "        assert len(filtered_search_results) > 0  # There should be at least one result\n",
    "    else:\n",
    "        assert len(filtered_search_results) == 0  # There should be no results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646a25f",
   "metadata": {},
   "source": [
    "It is also possible to provide `k`, which is the upper limit of the number of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e53d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_filter = Filter.by_property(\"source\").equal(\"state_of_the_union.txt\")\n",
    "filtered_search_results = db.similarity_search(query, filters=search_filter, k=3)\n",
    "assert len(filtered_search_results) <= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfd9bc",
   "metadata": {},
   "source": [
    "### Quantify result similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b286d60",
   "metadata": {},
   "source": [
    "You can optionally retrieve a relevance \"score\". This is a relative score that indicates how good the particular search results is, amongst the pool of search results. \n",
    "\n",
    "Note that this is relative score, meaning that it should not be used to determine thresholds for relevance. However, it can be used to compare the relevance of different search results within the entire search result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b4a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909 : For that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to prot...\n",
      "0.700 : And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the h...\n",
      "0.648 : If you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. \n",
      "\n",
      "It won’t loo...\n",
      "0.630 : And my report is this: the State of the Union is strong—because you, the American people, are strong...\n",
      "0.603 : Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers tur...\n"
     ]
    }
   ],
   "source": [
    "docs = db.similarity_search_with_score(\"country\", k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc[1]:.3f}\", \":\", doc[0].page_content[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf9adc",
   "metadata": {},
   "source": [
    "## Search mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5b24a",
   "metadata": {},
   "source": [
    "`similarity_search` uses Weaviate's [hybrid search](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid).\n",
    "\n",
    "A hybrid search combines a vector and a keyword search, with `alpha` as the weight of the vector search. The `similarity_search` function allows you to pass additional arguments as kwargs. See this [reference doc](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid) for the available arguments.\n",
    "\n",
    "So, you can perform a pure keyword search by adding `alpha=0` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74a7bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the president say about Ketanji Brown Jackson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'state_of_the_union.txt'}, page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(query)\n",
    "docs = db.similarity_search(query, alpha=0)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b75761",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f298bc0",
   "metadata": {},
   "source": [
    "Any data added through `langchain-weaviate` will persist in Weaviate according to its configuration. \n",
    "\n",
    "WCS instances, for example, are configured to persist data indefinitely, and Docker instances can be set up to persist data in a volume. Read more about [Weaviate's persistence](https://weaviate.io/developers/weaviate/configuration/persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da874a61",
   "metadata": {},
   "source": [
    "## Multi-tenancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0719f",
   "metadata": {},
   "source": [
    "[Multi-tenancy](https://weaviate.io/developers/weaviate/concepts/data#multi-tenancy) allows you to have a high number of isolated collections of data, with the same collection configuration, in a single Weaviate instance. This is great for multi-user environments such as building a SaaS app, where each end user will have their own isolated data collection.\n",
    "\n",
    "To use multi-tenancy, the vector store need to be aware of the `tenant` parameter. \n",
    "\n",
    "So when adding any data, provide the `tenant` parameter as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d365855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-Mar-26 03:40 PM - langchain_weaviate.vectorstores - INFO - Tenant Foo does not exist in index LangChain_30b9273d43b3492db4fb2aba2e0d6871. Creating tenant.\n"
     ]
    }
   ],
   "source": [
    "db_with_mt = WeaviateVectorStore.from_documents(\n",
    "    docs, embeddings, client=weaviate_client, tenant=\"Foo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e6107",
   "metadata": {},
   "source": [
    "And when performing queries, provide the `tenant` parameter also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49659eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),\n",
       " Document(page_content='And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room.', metadata={'source': 'state_of_the_union.txt'}),\n",
       " Document(page_content='He and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does.', metadata={'source': 'state_of_the_union.txt'}),\n",
       " Document(page_content='Putin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.', metadata={'source': 'state_of_the_union.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_with_mt.similarity_search(query, tenant=\"Foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ecf858",
   "metadata": {},
   "source": [
    "## Retriever options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3757a",
   "metadata": {},
   "source": [
    "Weaviate can also be used as a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8712d",
   "metadata": {},
   "source": [
    "### Maximal marginal relevance search (MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92add51",
   "metadata": {},
   "source": [
    "In addition to using similaritysearch  in the retriever object, you can also use `mmr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb302651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# retriever = db.as_retriever(search_type=\"mmr\")\n",
    "# retriever.invoke(query)[0]\n",
    "# del retriever\n",
    "print(\"--\")\n",
    "retriever = db.as_retriever(search_type=\"similarity\")\n",
    "retriever.invoke(query)[0]\n",
    "del retriever\n",
    "print(\"--\")\n",
    "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs=dict(score_threshold=0.5))\n",
    "retriever.invoke(query)[0]\n",
    "del retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f52ca4",
   "metadata": {},
   "source": [
    "# Use with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66690c78",
   "metadata": {},
   "source": [
    "A known limitation of large languag models (LLMs) is that their training data can be outdated, or not include the specific domain knowledge that you require.\n",
    "\n",
    "Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74e20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/workspaces/langchain-weaviate/.venv/lib/python3.12/site-packages/pydantic/main.py:1024: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot provide real-time information as my responses are generated based on a mixture of licensed data, data created by human trainers, and publicly available data. The last update was in October 2021.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm.predict(\"What did the president say about Justice Breyer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829720ad",
   "metadata": {},
   "source": [
    "Vector stores complement LLMs by providing a way to store and retrieve relevant information. This allow you to combine the strengths of LLMs and vector stores, by using LLM's reasoning and linguistic capabilities with vector stores' ability to retrieve relevant information.\n",
    "\n",
    "Two well-known applications for combining LLMs and vector stores are:\n",
    "- Question answering\n",
    "- Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae85eb4",
   "metadata": {},
   "source": [
    "### Question Answering with Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d8ba7",
   "metadata": {},
   "source": [
    "Question answering in langchain can be enhanced by the use of vector stores. Let's see how this can be done.\n",
    "\n",
    "This section uses the `RetrievalQAWithSourcesChain`, which does the lookup of the documents from an Index. \n",
    "\n",
    "First, we will chunk the text again and import them into the Weaviate vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad91ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2438d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0e106ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    client=weaviate_client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546bc958",
   "metadata": {},
   "source": [
    "Now we can construct the chain, with the retriever specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86bbb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8371444",
   "metadata": {},
   "source": [
    "And run the chain, to ask the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c38cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/5cwdhb1x589bngmsb8rfj4h80000gn/T/ipykernel_44911/2734026716.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': ' The president thanked Justice Stephen Breyer for his service and announced his nomination of Judge Ketanji Brown Jackson to the Supreme Court.\\n',\n",
       " 'sources': '31-pl, 32-pl'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\"question\": \"What did the president say about Justice Breyer\"},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aaacf4c0-577a-4500-8d09-b8119186af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x1141c8c90>\n"
     ]
    }
   ],
   "source": [
    "print(docsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecab3b5",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "Another very popular application of combining LLMs and vector stores is retrieval-augmented generation (RAG). This is a technique that uses a retriever to find relevant information from a vector store, and then uses an LLM to provide an output based on the retrieved data and a prompt.\n",
    "\n",
    "We begin with a similar setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33b0a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2ade6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    client=weaviate_client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))],\n",
    ")\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88724aa5-7bb1-4510-9c0c-1d77ebe7b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain_cab45a317f264b6db4c0101edfb7c1a9\n"
     ]
    }
   ],
   "source": [
    "print(docsearch._index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39413671",
   "metadata": {},
   "source": [
    "We need to construct a template for the RAG model so that the retrieved information will be populated in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "578570b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74982155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47abe3a",
   "metadata": {},
   "source": [
    "And running the cell, we get a very similar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe129bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The president honored Justice Stephen Breyer for his service to the country as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. The president also mentioned nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to continue Justice Breyer's legacy of excellence. Additionally, the president emphasized the importance of nominating someone to serve on the United States Supreme Court.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What did the president say about Justice Breyer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a2553",
   "metadata": {},
   "source": [
    "But note that since the template is upto you to construct, you can customize it to your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7417ac5",
   "metadata": {},
   "source": [
    "### Wrap-up & resources\n",
    "\n",
    "Weaviate is a scalable, production-ready vector store. \n",
    "\n",
    "This integration allows Weaviate to be used with LangChain to enhance the capabilities of large language models with a robust data store. Its scalability and production-readiness make it a great choice as a vector store for your LangChain applications, and it will reduce your time to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b00ae-9937-4073-b000-aad5d69c9ad3",
   "metadata": {},
   "source": [
    "### Delete a collection\n",
    "\n",
    "This assumes using on-prem version of Weaviate (docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ced8dea4-4dce-4a0c-b273-4850f06d6584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from pprint import pprint\n",
    "weaviate_client = weaviate.connect_to_local(\n",
    "    host=\"172.20.0.200\",\n",
    "    port=8080,\n",
    "    grpc_port=50051\n",
    ")\n",
    "collections = weaviate_client.collections.list_all()\n",
    "names = [collections[x].name for x in collections.keys()]\n",
    "print(names)\n",
    "weaviate_client.collections.delete(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4894ca-3f87-418c-88d2-75c07c6eb109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
