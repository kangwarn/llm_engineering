{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a314de-7d69-4347-9055-074326c66e6b",
   "metadata": {},
   "source": [
    "## Load the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b918179-8470-43ae-9592-cf8d0df2d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1167b-d02f-4183-8508-69a662ec46d8",
   "metadata": {},
   "source": [
    "## Load the API Key\n",
    "\n",
    "Note that since we are using the internal LLM service (llm.ai.broadcom.net). It uses vLLM which provides the OpenAPI style API, so all that we need to do is to call the OpenAI API. To obtain the API key for the internal LLM service, you follow the instruction [here](https://auth.esp.vmware.com/api-tokens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2171ca-20c4-4af0-82ad-27a2f97c2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI LLM\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('VMWARE_LLM_API_KEY', 'your-key-if-not-using-env')\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"meta-llama/Meta-Llama-3-70B-Instruct\", base_url=\"https://llm.ai.broadcom.net/api/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5e35b-74c3-49e4-992b-ce4f8464ae99",
   "metadata": {},
   "source": [
    "## Create a conversation chain and chat function for Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0ebcfb-fe20-402e-914c-e6db5b0c46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/t6wtklnx17j0_y3qjw4_k44w0000gp/T/ipykernel_14906/52380565.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/r1/t6wtklnx17j0_y3qjw4_k44w0000gp/T/ipykernel_14906/52380565.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory, callbacks=[StdOutCallbackHandler()])\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation chain with memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# Generator function for streaming chatbot responses\n",
    "def chat_with_bot_stream(user_message, history):\n",
    "    # Add user's message to the memory\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    \n",
    "    # Start streaming response\n",
    "    response = conversation.llm._call(user_message, stop=None)  # Directly call the LLM's method for streaming\n",
    "    bot_reply = \"\"\n",
    "    for chunk in response.split(\"\\n\"):\n",
    "        bot_reply += chunk\n",
    "        # Append to chat history and yield the intermediate output\n",
    "        history.append((user_message, bot_reply))\n",
    "        yield history, history\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation.invoke({\"input\": question})\n",
    "    history.append((question, result[\"response\"]))\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fb7d3-41bc-4b39-8618-4731f6907f08",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab41fdff-651c-434c-bcae-52f84134550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.12/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Test\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "with gr.Blocks() as gr_interface:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with LangChain + OpenAI (Streaming)\")\n",
    "    msg = gr.Textbox(placeholder=\"Type your message here...\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    \n",
    "    # Initialize chat history\n",
    "    state = gr.State([])\n",
    "\n",
    "    # Define interaction\n",
    "    msg.submit(chat, [msg, state], [chatbot, state])\n",
    "    clear_btn.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "# Run the Gradio app\n",
    "gr_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a9198-80b0-470b-8289-47a48ea6811e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
